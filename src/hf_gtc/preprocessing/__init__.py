"""Preprocessing recipes for HuggingFace data pipelines.

This module provides utilities for text preprocessing,
tokenization, data streaming, data augmentation, tokenizer training,
data quality assessment, synthetic data generation, and data filtering.

Examples:
    >>> from hf_gtc.preprocessing import preprocess_text, StreamConfig, AugmentConfig
    >>> preprocess_text("  Hello World  ")
    'hello world'
    >>> config = StreamConfig(batch_size=500)
    >>> config.batch_size
    500
    >>> aug_config = AugmentConfig(probability=0.2)
    >>> aug_config.probability
    0.2
    >>> from hf_gtc.preprocessing import DeduplicationMethod
    >>> DeduplicationMethod.EXACT_HASH.value
    'exact_hash'
    >>> from hf_gtc.preprocessing import GenerationMethod
    >>> GenerationMethod.SELF_INSTRUCT.value
    'self_instruct'
    >>> from hf_gtc.preprocessing import FilterType, PIIType
    >>> FilterType.TOXICITY.value
    'toxicity'
    >>> PIIType.EMAIL.value
    'email'
"""

from __future__ import annotations

from hf_gtc.preprocessing.augmentation import (
    VALID_AUGMENTATION_LEVELS,
    VALID_AUGMENTATION_TYPES,
    VALID_NOISE_TYPES,
    AugmentationConfig,
    AugmentationLevel,
    AugmentationStats,
    AugmentationType,
    AugmentConfig,
    AugmentResult,
    BacktranslationConfig,
    NoiseConfig,
    NoiseType,
    SynonymConfig,
    apply_augmentation,
    augment_batch,
    augment_text,
    calculate_augmentation_factor,
    chain_augmentations,
    compute_augmentation_stats,
    create_augmentation_config,
    create_augmenter,
    create_backtranslation_config,
    create_noise_config,
    create_synonym_config,
    estimate_diversity_gain,
    format_augmentation_stats,
    get_augmentation_level,
    get_augmentation_type,
    get_noise_type,
    get_recommended_augmentation_config,
    inject_noise,
    list_augmentation_levels,
    list_augmentation_types,
    list_noise_types,
    random_delete,
    random_insert,
    random_swap,
    synonym_replace,
    validate_augment_config,
    validate_augmentation_config,
    validate_augmentation_level,
    validate_augmentation_type,
    validate_backtranslation_config,
    validate_noise_config,
    validate_noise_type,
    validate_synonym_config,
)
from hf_gtc.preprocessing.datasets import (
    DatasetInfo,
    create_train_test_split,
    create_train_val_test_split,
    filter_by_length,
    get_dataset_info,
    rename_columns,
    sample_dataset,
    select_columns,
)
from hf_gtc.preprocessing.filtering import (
    VALID_FILTER_TYPES,
    VALID_PII_TYPES,
    VALID_TOXICITY_CATEGORIES,
    FilterConfig,
    FilterResult,
    FilterStats,
    FilterType,
    LanguageConfig,
    PIIConfig,
    PIIType,
    ToxicityCategory,
    ToxicityConfig,
    apply_filters,
    create_filter_config,
    create_language_config,
    create_pii_config,
    create_toxicity_config,
    detect_language,
    detect_pii,
    detect_toxicity,
    format_filter_stats,
    get_filter_type,
    get_pii_type,
    get_recommended_filter_config,
    get_toxicity_category,
    list_filter_types,
    list_pii_types,
    list_toxicity_categories,
    redact_pii,
    validate_filter_config,
    validate_language_config,
    validate_pii_config,
    validate_toxicity_config,
)
from hf_gtc.preprocessing.quality import (
    VALID_DEDUPLICATION_METHODS,
    VALID_FILTER_STRATEGIES,
    VALID_QUALITY_METRICS,
    ContaminationConfig,
    DeduplicationConfig,
    DeduplicationMethod,
    FilterStrategy,
    QualityFilterConfig,
    QualityMetric,
    QualityStats,
    calculate_perplexity_score,
    calculate_text_quality_score,
    check_contamination,
    compute_quality_stats,
    create_contamination_config,
    create_deduplication_config,
    create_quality_filter_config,
    detect_duplicates,
    format_quality_stats,
    get_deduplication_method,
    get_filter_strategy,
    get_quality_metric,
    get_recommended_quality_config,
    list_deduplication_methods,
    list_filter_strategies,
    list_quality_metrics,
    validate_contamination_config,
    validate_deduplication_config,
    validate_quality_filter_config,
)
from hf_gtc.preprocessing.streaming import (
    ShuffleMode,
    StreamConfig,
    StreamProgress,
    StreamStats,
    compute_stream_stats,
    create_stream_iterator,
    filter_stream,
    list_shuffle_modes,
    map_stream,
    skip_stream,
    stream_batches,
    stream_dataset,
    take_stream,
    validate_shuffle_mode,
    validate_stream_config,
)
from hf_gtc.preprocessing.synthetic import (
    VALID_DIVERSITY_STRATEGIES,
    VALID_GENERATION_METHODS,
    VALID_QUALITY_FILTERS,
    DiversityStrategy,
    EvolInstructConfig,
    GenerationMethod,
    GenerationStats,
    QualityFilter,
    SelfInstructConfig,
    SyntheticConfig,
    SyntheticSample,
    calculate_diversity_score,
    compute_generation_stats,
    create_evol_instruct_config,
    create_self_instruct_config,
    create_synthetic_config,
    deduplicate_samples,
    estimate_generation_cost,
    filter_synthetic_samples,
    format_generation_stats,
    get_diversity_strategy,
    get_generation_method,
    get_quality_filter,
    get_recommended_synthetic_config,
    list_diversity_strategies,
    list_generation_methods,
    list_quality_filters,
    validate_evol_instruct_config,
    validate_self_instruct_config,
    validate_synthetic_config,
    validate_synthetic_quality,
)
from hf_gtc.preprocessing.tokenization import (
    VALID_SPECIAL_TOKEN_TYPES,
    VALID_TOKENIZER_TYPES,
    VALID_VOCAB_ANALYSIS_METRICS,
    SpecialTokenType,
    TokenizationResult,
    TokenizerConfig,
    TokenizerType,
    VocabAnalysisMetric,
    VocabStats,
    analyze_vocabulary,
    calculate_fertility,
    compare_tokenizers,
    create_preprocessing_function,
    create_tokenizer_config,
    create_vocab_stats,
    detect_special_tokens,
    estimate_sequence_length,
    format_vocab_stats,
    get_recommended_tokenizer_config,
    get_special_token_type,
    get_tokenizer_type,
    get_vocab_analysis_metric,
    list_special_token_types,
    list_tokenizer_types,
    list_vocab_analysis_metrics,
    preprocess_text,
    tokenize_batch,
    validate_tokenization_result,
    validate_tokenizer_config,
    validate_vocab_stats,
)
from hf_gtc.preprocessing.tokenizer_training import (
    VALID_ALGORITHMS,
    VALID_NORMALIZERS,
    VALID_PRE_TOKENIZERS,
    BPEConfig,
    PreTokenizerConfig,
    PreTokenizerType,
    TokenizerAlgorithm,
    TrainingCorpusConfig,
    UnigramConfig,
    WordPieceConfig,
    calculate_compression_ratio,
    create_bpe_config,
    create_pre_tokenizer_config,
    create_training_corpus_config,
    create_unigram_config,
    create_wordpiece_config,
    estimate_vocab_coverage,
    get_recommended_algorithm,
    get_tokenizer_algorithm,
    list_pre_tokenizers,
    list_tokenizer_algorithms,
    validate_bpe_config,
    validate_unigram_config,
    validate_wordpiece_config,
)

__all__: list[str] = [
    "VALID_ALGORITHMS",
    "VALID_AUGMENTATION_LEVELS",
    "VALID_AUGMENTATION_TYPES",
    "VALID_DEDUPLICATION_METHODS",
    "VALID_DIVERSITY_STRATEGIES",
    "VALID_FILTER_STRATEGIES",
    "VALID_FILTER_TYPES",
    "VALID_GENERATION_METHODS",
    "VALID_NOISE_TYPES",
    "VALID_NORMALIZERS",
    "VALID_PII_TYPES",
    "VALID_PRE_TOKENIZERS",
    "VALID_QUALITY_FILTERS",
    "VALID_QUALITY_METRICS",
    "VALID_SPECIAL_TOKEN_TYPES",
    "VALID_TOKENIZER_TYPES",
    "VALID_TOXICITY_CATEGORIES",
    "VALID_VOCAB_ANALYSIS_METRICS",
    "AugmentConfig",
    "AugmentResult",
    "AugmentationConfig",
    "AugmentationLevel",
    "AugmentationStats",
    "AugmentationType",
    "BPEConfig",
    "BacktranslationConfig",
    "ContaminationConfig",
    "DatasetInfo",
    "DeduplicationConfig",
    "DeduplicationMethod",
    "DiversityStrategy",
    "EvolInstructConfig",
    "FilterConfig",
    "FilterResult",
    "FilterStats",
    "FilterStrategy",
    "FilterType",
    "GenerationMethod",
    "GenerationStats",
    "LanguageConfig",
    "NoiseConfig",
    "NoiseType",
    "PIIConfig",
    "PIIType",
    "PreTokenizerConfig",
    "PreTokenizerType",
    "QualityFilter",
    "QualityFilterConfig",
    "QualityMetric",
    "QualityStats",
    "SelfInstructConfig",
    "ShuffleMode",
    "SpecialTokenType",
    "StreamConfig",
    "StreamProgress",
    "StreamStats",
    "SynonymConfig",
    "SyntheticConfig",
    "SyntheticSample",
    "TokenizationResult",
    "TokenizerAlgorithm",
    "TokenizerConfig",
    "TokenizerType",
    "ToxicityCategory",
    "ToxicityConfig",
    "TrainingCorpusConfig",
    "UnigramConfig",
    "VocabAnalysisMetric",
    "VocabStats",
    "WordPieceConfig",
    "analyze_vocabulary",
    "apply_augmentation",
    "apply_filters",
    "augment_batch",
    "augment_text",
    "calculate_augmentation_factor",
    "calculate_compression_ratio",
    "calculate_diversity_score",
    "calculate_fertility",
    "calculate_perplexity_score",
    "calculate_text_quality_score",
    "chain_augmentations",
    "check_contamination",
    "compare_tokenizers",
    "compute_augmentation_stats",
    "compute_generation_stats",
    "compute_quality_stats",
    "compute_stream_stats",
    "create_augmentation_config",
    "create_augmenter",
    "create_backtranslation_config",
    "create_bpe_config",
    "create_contamination_config",
    "create_deduplication_config",
    "create_evol_instruct_config",
    "create_filter_config",
    "create_language_config",
    "create_noise_config",
    "create_pii_config",
    "create_pre_tokenizer_config",
    "create_preprocessing_function",
    "create_quality_filter_config",
    "create_self_instruct_config",
    "create_stream_iterator",
    "create_synonym_config",
    "create_synthetic_config",
    "create_tokenizer_config",
    "create_toxicity_config",
    "create_train_test_split",
    "create_train_val_test_split",
    "create_training_corpus_config",
    "create_unigram_config",
    "create_vocab_stats",
    "create_wordpiece_config",
    "deduplicate_samples",
    "detect_duplicates",
    "detect_language",
    "detect_pii",
    "detect_special_tokens",
    "detect_toxicity",
    "estimate_diversity_gain",
    "estimate_generation_cost",
    "estimate_sequence_length",
    "estimate_vocab_coverage",
    "filter_by_length",
    "filter_stream",
    "filter_synthetic_samples",
    "format_augmentation_stats",
    "format_filter_stats",
    "format_generation_stats",
    "format_quality_stats",
    "format_vocab_stats",
    "get_augmentation_level",
    "get_augmentation_type",
    "get_dataset_info",
    "get_deduplication_method",
    "get_diversity_strategy",
    "get_filter_strategy",
    "get_filter_type",
    "get_generation_method",
    "get_noise_type",
    "get_pii_type",
    "get_quality_filter",
    "get_quality_metric",
    "get_recommended_algorithm",
    "get_recommended_augmentation_config",
    "get_recommended_filter_config",
    "get_recommended_quality_config",
    "get_recommended_synthetic_config",
    "get_recommended_tokenizer_config",
    "get_special_token_type",
    "get_tokenizer_algorithm",
    "get_tokenizer_type",
    "get_toxicity_category",
    "get_vocab_analysis_metric",
    "inject_noise",
    "list_augmentation_levels",
    "list_augmentation_types",
    "list_deduplication_methods",
    "list_diversity_strategies",
    "list_filter_strategies",
    "list_filter_types",
    "list_generation_methods",
    "list_noise_types",
    "list_pii_types",
    "list_pre_tokenizers",
    "list_quality_filters",
    "list_quality_metrics",
    "list_shuffle_modes",
    "list_special_token_types",
    "list_tokenizer_algorithms",
    "list_tokenizer_types",
    "list_toxicity_categories",
    "list_vocab_analysis_metrics",
    "map_stream",
    "preprocess_text",
    "random_delete",
    "random_insert",
    "random_swap",
    "redact_pii",
    "rename_columns",
    "sample_dataset",
    "select_columns",
    "skip_stream",
    "stream_batches",
    "stream_dataset",
    "synonym_replace",
    "take_stream",
    "tokenize_batch",
    "validate_augment_config",
    "validate_augmentation_config",
    "validate_augmentation_level",
    "validate_augmentation_type",
    "validate_backtranslation_config",
    "validate_bpe_config",
    "validate_contamination_config",
    "validate_deduplication_config",
    "validate_evol_instruct_config",
    "validate_filter_config",
    "validate_language_config",
    "validate_noise_config",
    "validate_noise_type",
    "validate_pii_config",
    "validate_quality_filter_config",
    "validate_self_instruct_config",
    "validate_shuffle_mode",
    "validate_stream_config",
    "validate_synonym_config",
    "validate_synthetic_config",
    "validate_synthetic_quality",
    "validate_tokenization_result",
    "validate_tokenizer_config",
    "validate_toxicity_config",
    "validate_unigram_config",
    "validate_vocab_stats",
    "validate_wordpiece_config",
]
