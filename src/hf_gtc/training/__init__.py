"""Training recipes for HuggingFace models.

This module provides utilities for model training using
the HuggingFace Trainer API, PEFT for parameter-efficient fine-tuning,
and distributed training strategies.

Examples:
    >>> from hf_gtc.training import TrainingConfig, TrainerState
    >>> config = TrainingConfig(output_dir="/tmp/test")
    >>> config.num_epochs
    3
    >>> state = TrainerState(global_step=100, epoch=1.5)
    >>> state.global_step
    100
"""

from __future__ import annotations

from hf_gtc.training.callbacks import (
    CallbackMetrics,
    CheckpointConfig,
    EarlyStoppingConfig,
    LoggingConfig,
    MetricMode,
    create_early_stopping_callback,
    create_logging_callback,
    get_recommended_callbacks,
    list_callback_types,
    should_stop_early,
    validate_checkpoint_config,
    validate_early_stopping_config,
    validate_logging_config,
)
from hf_gtc.training.distributed import (
    VALID_BACKENDS,
    VALID_CHECKPOINTING,
    VALID_DEEPSPEED_STAGES,
    VALID_SHARDING_STRATEGIES,
    ActivationCheckpointing,
    DeepSpeedConfig,
    DeepSpeedStage,
    DistributedBackend,
    DistributedConfig,
    FSDPConfig,
    MemoryEstimate,
    ScalingMetrics,
    ShardingStrategy,
    calculate_scaling_efficiency,
    create_deepspeed_config,
    create_distributed_config,
    create_fsdp_config,
    estimate_deepspeed_memory,
    estimate_fsdp_memory,
    format_memory_estimate,
    get_deepspeed_stage,
    get_recommended_strategy,
    get_sharding_strategy,
    list_backends,
    list_deepspeed_stages,
    list_sharding_strategies,
    validate_deepspeed_config,
    validate_distributed_config,
    validate_fsdp_config,
)
from hf_gtc.training.dpo import (
    VALID_DPO_VARIANTS,
    VALID_LOSS_TYPES,
    VALID_REFERENCE_POLICIES,
    DPOConfig,
    DPOStats,
    DPOTrainingConfig,
    DPOVariant,
    LossType,
    PreferencePair,
    ReferenceConfig,
    ReferencePolicy,
    calculate_dpo_loss,
    create_dpo_config,
    create_dpo_training_config,
    create_preference_pair,
    create_reference_config,
    estimate_training_steps,
    get_dpo_variant,
    get_loss_type,
    get_reference_policy,
    list_dpo_variants,
    list_loss_types,
    list_reference_policies,
    validate_dpo_config,
    validate_preference_pair,
)
from hf_gtc.training.fine_tuning import (
    TrainingConfig,
    compute_num_training_steps,
    create_trainer,
    create_training_args,
    validate_training_config,
)
from hf_gtc.training.lora import (
    LoRAConfig,
    TaskType,
    calculate_lora_memory_savings,
    create_lora_config,
    estimate_lora_parameters,
    get_peft_config,
    get_recommended_lora_config,
    list_task_types,
)
from hf_gtc.training.ppo import (
    VALID_PPO_VARIANTS,
    VALID_REWARD_MODEL_TYPES,
    VALID_VALUE_HEAD_TYPES,
    PPOConfig,
    PPOStats,
    PPOTrainingConfig,
    PPOVariant,
    RewardConfig,
    RewardModelType,
    ValueConfig,
    ValueHeadType,
    calculate_gae,
    calculate_ppo_loss,
    create_ppo_config,
    create_ppo_training_config,
    create_reward_config,
    create_value_config,
    get_ppo_variant,
    get_reward_model_type,
    get_value_head_type,
    list_ppo_variants,
    list_reward_model_types,
    list_value_head_types,
    validate_ppo_config,
    validate_ppo_training_config,
    validate_reward_config,
    validate_value_config,
)
from hf_gtc.training.qlora import (
    ComputeType,
    QLoRAConfig,
    QLoRATrainingConfig,
    QuantConfig,
    QuantType,
    calculate_qlora_trainable_params,
    create_qlora_config,
    create_qlora_training_config,
    create_quant_config,
    estimate_qlora_memory,
    get_bnb_config,
    get_compute_type,
    get_qlora_peft_config,
    get_quant_type,
    get_recommended_qlora_config,
    list_compute_types,
    list_quant_bits,
    list_quant_types,
    validate_compute_type,
    validate_qlora_config,
    validate_quant_config,
    validate_quant_type,
)

# Note: MemoryEstimate is from distributed module, format_memory_estimate as well
# (qlora has its own format_memory_estimate which we use a different name for)
from hf_gtc.training.qlora import format_memory_estimate as format_qlora_memory_estimate
from hf_gtc.training.trainer import (
    SchedulerConfig,
    SchedulerType,
    TrainerState,
    TrainingProgress,
    compute_warmup_steps,
    create_trainer_state,
    create_training_progress,
    format_training_progress,
    get_checkpoint_path,
    get_checkpoints_to_delete,
    get_early_stopping_mode,
    get_latest_checkpoint,
    get_scheduler_type,
    is_metric_improved,
    list_checkpoints,
    list_early_stopping_modes,
    list_scheduler_types,
    update_trainer_state,
    validate_scheduler_config,
    validate_scheduler_type,
    validate_trainer_state,
)

__all__: list[str] = [
    "VALID_BACKENDS",
    "VALID_CHECKPOINTING",
    "VALID_DEEPSPEED_STAGES",
    "VALID_DPO_VARIANTS",
    "VALID_LOSS_TYPES",
    "VALID_PPO_VARIANTS",
    "VALID_REFERENCE_POLICIES",
    "VALID_REWARD_MODEL_TYPES",
    "VALID_SHARDING_STRATEGIES",
    "VALID_VALUE_HEAD_TYPES",
    "ActivationCheckpointing",
    "CallbackMetrics",
    "CheckpointConfig",
    "ComputeType",
    "DPOConfig",
    "DPOStats",
    "DPOTrainingConfig",
    "DPOVariant",
    "DeepSpeedConfig",
    "DeepSpeedStage",
    "DistributedBackend",
    "DistributedConfig",
    "EarlyStoppingConfig",
    "FSDPConfig",
    "LoRAConfig",
    "LoggingConfig",
    "LossType",
    "MemoryEstimate",
    "MetricMode",
    "PPOConfig",
    "PPOStats",
    "PPOTrainingConfig",
    "PPOVariant",
    "PreferencePair",
    "QLoRAConfig",
    "QLoRATrainingConfig",
    "QuantConfig",
    "QuantType",
    "ReferenceConfig",
    "ReferencePolicy",
    "RewardConfig",
    "RewardModelType",
    "ScalingMetrics",
    "SchedulerConfig",
    "SchedulerType",
    "ShardingStrategy",
    "TaskType",
    "TrainerState",
    "TrainingConfig",
    "TrainingProgress",
    "ValueConfig",
    "ValueHeadType",
    "calculate_dpo_loss",
    "calculate_gae",
    "calculate_lora_memory_savings",
    "calculate_ppo_loss",
    "calculate_qlora_trainable_params",
    "calculate_scaling_efficiency",
    "compute_num_training_steps",
    "compute_warmup_steps",
    "create_deepspeed_config",
    "create_distributed_config",
    "create_dpo_config",
    "create_dpo_training_config",
    "create_early_stopping_callback",
    "create_fsdp_config",
    "create_logging_callback",
    "create_lora_config",
    "create_ppo_config",
    "create_ppo_training_config",
    "create_preference_pair",
    "create_qlora_config",
    "create_qlora_training_config",
    "create_quant_config",
    "create_reference_config",
    "create_reward_config",
    "create_trainer",
    "create_trainer_state",
    "create_training_args",
    "create_training_progress",
    "create_value_config",
    "estimate_deepspeed_memory",
    "estimate_fsdp_memory",
    "estimate_lora_parameters",
    "estimate_qlora_memory",
    "estimate_training_steps",
    "format_memory_estimate",
    "format_qlora_memory_estimate",
    "format_training_progress",
    "get_bnb_config",
    "get_checkpoint_path",
    "get_checkpoints_to_delete",
    "get_compute_type",
    "get_deepspeed_stage",
    "get_dpo_variant",
    "get_early_stopping_mode",
    "get_latest_checkpoint",
    "get_loss_type",
    "get_peft_config",
    "get_ppo_variant",
    "get_qlora_peft_config",
    "get_quant_type",
    "get_recommended_callbacks",
    "get_recommended_lora_config",
    "get_recommended_qlora_config",
    "get_recommended_strategy",
    "get_reference_policy",
    "get_reward_model_type",
    "get_scheduler_type",
    "get_sharding_strategy",
    "get_value_head_type",
    "is_metric_improved",
    "list_backends",
    "list_callback_types",
    "list_checkpoints",
    "list_compute_types",
    "list_deepspeed_stages",
    "list_dpo_variants",
    "list_early_stopping_modes",
    "list_loss_types",
    "list_ppo_variants",
    "list_quant_bits",
    "list_quant_types",
    "list_reference_policies",
    "list_reward_model_types",
    "list_scheduler_types",
    "list_sharding_strategies",
    "list_task_types",
    "list_value_head_types",
    "should_stop_early",
    "update_trainer_state",
    "validate_checkpoint_config",
    "validate_compute_type",
    "validate_deepspeed_config",
    "validate_distributed_config",
    "validate_dpo_config",
    "validate_early_stopping_config",
    "validate_fsdp_config",
    "validate_logging_config",
    "validate_ppo_config",
    "validate_ppo_training_config",
    "validate_preference_pair",
    "validate_qlora_config",
    "validate_quant_config",
    "validate_quant_type",
    "validate_reward_config",
    "validate_scheduler_config",
    "validate_scheduler_type",
    "validate_trainer_state",
    "validate_training_config",
    "validate_value_config",
]
