"""Training recipes for HuggingFace models.

This module provides utilities for model training using
the HuggingFace Trainer API, PEFT for parameter-efficient fine-tuning,
and distributed training strategies.

Examples:
    >>> from hf_gtc.training import TrainingConfig, TrainerState
    >>> config = TrainingConfig(output_dir="/tmp/test")
    >>> config.num_epochs
    3
    >>> state = TrainerState(global_step=100, epoch=1.5)
    >>> state.global_step
    100
"""

from __future__ import annotations

from hf_gtc.training.callbacks import (
    CallbackMetrics,
    CheckpointConfig,
    EarlyStoppingConfig,
    LoggingConfig,
    MetricMode,
    create_early_stopping_callback,
    create_logging_callback,
    get_recommended_callbacks,
    list_callback_types,
    should_stop_early,
    validate_checkpoint_config,
    validate_early_stopping_config,
    validate_logging_config,
)
from hf_gtc.training.distributed import (
    VALID_BACKENDS,
    VALID_CHECKPOINTING,
    VALID_DEEPSPEED_STAGES,
    VALID_SHARDING_STRATEGIES,
    ActivationCheckpointing,
    DeepSpeedConfig,
    DeepSpeedStage,
    DistributedBackend,
    DistributedConfig,
    FSDPConfig,
    MemoryEstimate,
    ScalingMetrics,
    ShardingStrategy,
    calculate_scaling_efficiency,
    create_deepspeed_config,
    create_distributed_config,
    create_fsdp_config,
    estimate_deepspeed_memory,
    estimate_fsdp_memory,
    format_memory_estimate,
    get_deepspeed_stage,
    get_recommended_strategy,
    get_sharding_strategy,
    list_backends,
    list_deepspeed_stages,
    list_sharding_strategies,
    validate_deepspeed_config,
    validate_distributed_config,
    validate_fsdp_config,
)
from hf_gtc.training.fine_tuning import (
    TrainingConfig,
    compute_num_training_steps,
    create_trainer,
    create_training_args,
    validate_training_config,
)
from hf_gtc.training.lora import (
    LoRAConfig,
    TaskType,
    calculate_lora_memory_savings,
    create_lora_config,
    estimate_lora_parameters,
    get_peft_config,
    get_recommended_lora_config,
    list_task_types,
)
from hf_gtc.training.qlora import (
    ComputeType,
    QLoRAConfig,
    QLoRATrainingConfig,
    QuantConfig,
    QuantType,
    calculate_qlora_trainable_params,
    create_qlora_config,
    create_qlora_training_config,
    create_quant_config,
    estimate_qlora_memory,
    get_bnb_config,
    get_compute_type,
    get_qlora_peft_config,
    get_quant_type,
    get_recommended_qlora_config,
    list_compute_types,
    list_quant_bits,
    list_quant_types,
    validate_compute_type,
    validate_qlora_config,
    validate_quant_config,
    validate_quant_type,
)

# Note: MemoryEstimate is from distributed module, format_memory_estimate as well
# (qlora has its own format_memory_estimate which we use a different name for)
from hf_gtc.training.qlora import format_memory_estimate as format_qlora_memory_estimate
from hf_gtc.training.trainer import (
    SchedulerConfig,
    SchedulerType,
    TrainerState,
    TrainingProgress,
    compute_warmup_steps,
    create_trainer_state,
    create_training_progress,
    format_training_progress,
    get_checkpoint_path,
    get_checkpoints_to_delete,
    get_early_stopping_mode,
    get_latest_checkpoint,
    get_scheduler_type,
    is_metric_improved,
    list_checkpoints,
    list_early_stopping_modes,
    list_scheduler_types,
    update_trainer_state,
    validate_scheduler_config,
    validate_scheduler_type,
    validate_trainer_state,
)

__all__: list[str] = [
    "VALID_BACKENDS",
    "VALID_CHECKPOINTING",
    "VALID_DEEPSPEED_STAGES",
    "VALID_SHARDING_STRATEGIES",
    "ActivationCheckpointing",
    "CallbackMetrics",
    "CheckpointConfig",
    "ComputeType",
    "DeepSpeedConfig",
    "DeepSpeedStage",
    "DistributedBackend",
    "DistributedConfig",
    "EarlyStoppingConfig",
    "FSDPConfig",
    "LoRAConfig",
    "LoggingConfig",
    "MemoryEstimate",
    "MetricMode",
    "QLoRAConfig",
    "QLoRATrainingConfig",
    "QuantConfig",
    "QuantType",
    "ScalingMetrics",
    "SchedulerConfig",
    "SchedulerType",
    "ShardingStrategy",
    "TaskType",
    "TrainerState",
    "TrainingConfig",
    "TrainingProgress",
    "calculate_lora_memory_savings",
    "calculate_qlora_trainable_params",
    "calculate_scaling_efficiency",
    "compute_num_training_steps",
    "compute_warmup_steps",
    "create_deepspeed_config",
    "create_distributed_config",
    "create_early_stopping_callback",
    "create_fsdp_config",
    "create_logging_callback",
    "create_lora_config",
    "create_qlora_config",
    "create_qlora_training_config",
    "create_quant_config",
    "create_trainer",
    "create_trainer_state",
    "create_training_args",
    "create_training_progress",
    "estimate_deepspeed_memory",
    "estimate_fsdp_memory",
    "estimate_lora_parameters",
    "estimate_qlora_memory",
    "format_memory_estimate",
    "format_qlora_memory_estimate",
    "format_training_progress",
    "get_bnb_config",
    "get_checkpoint_path",
    "get_checkpoints_to_delete",
    "get_compute_type",
    "get_deepspeed_stage",
    "get_early_stopping_mode",
    "get_latest_checkpoint",
    "get_peft_config",
    "get_qlora_peft_config",
    "get_quant_type",
    "get_recommended_callbacks",
    "get_recommended_lora_config",
    "get_recommended_qlora_config",
    "get_recommended_strategy",
    "get_scheduler_type",
    "get_sharding_strategy",
    "is_metric_improved",
    "list_backends",
    "list_callback_types",
    "list_checkpoints",
    "list_compute_types",
    "list_deepspeed_stages",
    "list_early_stopping_modes",
    "list_quant_bits",
    "list_quant_types",
    "list_scheduler_types",
    "list_sharding_strategies",
    "list_task_types",
    "should_stop_early",
    "update_trainer_state",
    "validate_checkpoint_config",
    "validate_compute_type",
    "validate_deepspeed_config",
    "validate_distributed_config",
    "validate_early_stopping_config",
    "validate_fsdp_config",
    "validate_logging_config",
    "validate_qlora_config",
    "validate_quant_config",
    "validate_quant_type",
    "validate_scheduler_config",
    "validate_scheduler_type",
    "validate_trainer_state",
    "validate_training_config",
]
