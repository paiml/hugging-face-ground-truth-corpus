"""Inference recipes for HuggingFace models.

This module provides utilities for creating pipelines, batch inference,
device placement, and inference optimizations.

Examples:
    >>> from hf_gtc.inference import get_device, BatchConfig
    >>> device = get_device()
    >>> device in ("cuda", "mps", "cpu")
    True
    >>> config = BatchConfig(batch_size=32)
    >>> config.batch_size
    32
"""

from __future__ import annotations

from hf_gtc.inference.batch import (
    BatchConfig,
    BatchResult,
    BatchStats,
    PaddingStrategy,
    compute_batch_stats,
    compute_num_batches,
    create_batches,
    estimate_memory_per_batch,
    get_optimal_batch_size,
    list_padding_strategies,
    validate_batch_config,
)
from hf_gtc.inference.device import (
    clear_gpu_memory,
    get_device,
    get_device_map,
    get_gpu_memory_info,
)
from hf_gtc.inference.optimization import (
    VALID_ATTENTION_IMPLS,
    VALID_CACHE_TYPES,
    VALID_KV_QUANT_TYPES,
    AttentionImplementation,
    ContinuousBatchingConfig,
    FlashAttentionConfig,
    KVCacheConfig,
    KVCacheType,
    QuantizedKVConfig,
    SpeculativeDecodingConfig,
    calculate_speculative_speedup,
    create_continuous_batching_config,
    create_flash_attention_config,
    create_kv_cache_config,
    create_quantized_kv_config,
    create_speculative_decoding_config,
    estimate_kv_cache_memory,
    get_recommended_attention,
    list_attention_implementations,
    list_kv_cache_types,
    validate_kv_cache_config,
    validate_speculative_config,
)
from hf_gtc.inference.pipelines import create_pipeline, list_supported_tasks
from hf_gtc.inference.streaming import (
    VALID_CALLBACK_TYPES,
    VALID_OVERFLOW_STRATEGIES,
    VALID_STREAMING_MODES,
    BufferConfig,
    CallbackType,
    ChunkConfig,
    StreamConfig,
    StreamEvent,
    StreamingMode,
    StreamingStats,
    calculate_tokens_per_second,
    create_buffer_config,
    create_chunk_config,
    create_stream_config,
    estimate_stream_duration,
    get_callback_type,
    get_streaming_mode,
    list_callback_types,
    list_streaming_modes,
    validate_chunk_config,
    validate_stream_config,
)

__all__: list[str] = [
    "VALID_ATTENTION_IMPLS",
    "VALID_CACHE_TYPES",
    "VALID_CALLBACK_TYPES",
    "VALID_KV_QUANT_TYPES",
    "VALID_OVERFLOW_STRATEGIES",
    "VALID_STREAMING_MODES",
    "AttentionImplementation",
    "BatchConfig",
    "BatchResult",
    "BatchStats",
    "BufferConfig",
    "CallbackType",
    "ChunkConfig",
    "ContinuousBatchingConfig",
    "FlashAttentionConfig",
    "KVCacheConfig",
    "KVCacheType",
    "PaddingStrategy",
    "QuantizedKVConfig",
    "SpeculativeDecodingConfig",
    "StreamConfig",
    "StreamEvent",
    "StreamingMode",
    "StreamingStats",
    "calculate_speculative_speedup",
    "calculate_tokens_per_second",
    "clear_gpu_memory",
    "compute_batch_stats",
    "compute_num_batches",
    "create_batches",
    "create_buffer_config",
    "create_chunk_config",
    "create_continuous_batching_config",
    "create_flash_attention_config",
    "create_kv_cache_config",
    "create_pipeline",
    "create_quantized_kv_config",
    "create_speculative_decoding_config",
    "create_stream_config",
    "estimate_kv_cache_memory",
    "estimate_memory_per_batch",
    "estimate_stream_duration",
    "get_callback_type",
    "get_device",
    "get_device_map",
    "get_gpu_memory_info",
    "get_optimal_batch_size",
    "get_recommended_attention",
    "get_streaming_mode",
    "list_attention_implementations",
    "list_callback_types",
    "list_kv_cache_types",
    "list_padding_strategies",
    "list_streaming_modes",
    "list_supported_tasks",
    "validate_batch_config",
    "validate_chunk_config",
    "validate_kv_cache_config",
    "validate_speculative_config",
    "validate_stream_config",
]
