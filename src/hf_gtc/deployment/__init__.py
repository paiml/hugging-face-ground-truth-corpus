"""Deployment recipes for HuggingFace models.

This module provides utilities for model quantization,
format conversion, and serving.

Examples:
    >>> from hf_gtc.deployment import ServerConfig, QuantizationType
    >>> config = ServerConfig(port=8080)
    >>> config.port
    8080
"""

from __future__ import annotations

from hf_gtc.deployment.optimization import (
    OptimizationResult,
    QuantizationConfig,
    QuantizationType,
    calculate_compression_ratio,
    estimate_model_size,
    get_model_loading_kwargs,
    get_optimization_result,
    get_quantization_config,
    list_quantization_types,
)
from hf_gtc.deployment.quantization import (
    AWQConfig,
    CalibrationConfig,
    CalibrationMethod,
    GPTQConfig,
    QuantGranularity,
    QuantMethod,
    QuantProfile,
    QuantResult,
    compute_compression_ratio,
    create_awq_config,
    create_calibration_config,
    create_gptq_config,
    create_quant_profile,
    create_quant_result,
    estimate_quantized_size,
    format_quant_result,
    get_awq_dict,
    get_calibration_method,
    get_gptq_dict,
    get_quant_granularity,
    get_quant_method,
    get_recommended_profile,
    list_calibration_methods,
    list_quant_granularities,
    list_quant_methods,
    validate_calibration_config,
    validate_calibration_method,
    validate_quant_granularity,
    validate_quant_method,
    validate_quant_profile,
)
from hf_gtc.deployment.serving import (
    HealthStatus,
    InferenceBackend,
    InferenceRequest,
    InferenceResponse,
    ModelServer,
    ServerConfig,
    ServerStatus,
    compute_server_metrics,
    create_server,
    format_server_info,
    get_health_status,
    get_inference_backend,
    get_server_status,
    list_inference_backends,
    list_server_statuses,
    process_batch,
    process_request,
    start_server,
    stop_server,
    validate_inference_backend,
    validate_server_config,
    validate_server_status,
)

__all__: list[str] = [
    "AWQConfig",
    "CalibrationConfig",
    "CalibrationMethod",
    "GPTQConfig",
    "HealthStatus",
    "InferenceBackend",
    "InferenceRequest",
    "InferenceResponse",
    "ModelServer",
    "OptimizationResult",
    "QuantGranularity",
    "QuantMethod",
    "QuantProfile",
    "QuantResult",
    "QuantizationConfig",
    "QuantizationType",
    "ServerConfig",
    "ServerStatus",
    "calculate_compression_ratio",
    "compute_compression_ratio",
    "compute_server_metrics",
    "create_awq_config",
    "create_calibration_config",
    "create_gptq_config",
    "create_quant_profile",
    "create_quant_result",
    "create_server",
    "estimate_model_size",
    "estimate_quantized_size",
    "format_quant_result",
    "format_server_info",
    "get_awq_dict",
    "get_calibration_method",
    "get_gptq_dict",
    "get_health_status",
    "get_inference_backend",
    "get_model_loading_kwargs",
    "get_optimization_result",
    "get_quant_granularity",
    "get_quant_method",
    "get_quantization_config",
    "get_recommended_profile",
    "get_server_status",
    "list_calibration_methods",
    "list_inference_backends",
    "list_quant_granularities",
    "list_quant_methods",
    "list_quantization_types",
    "list_server_statuses",
    "process_batch",
    "process_request",
    "start_server",
    "stop_server",
    "validate_calibration_config",
    "validate_calibration_method",
    "validate_inference_backend",
    "validate_quant_granularity",
    "validate_quant_method",
    "validate_quant_profile",
    "validate_server_config",
    "validate_server_status",
]
