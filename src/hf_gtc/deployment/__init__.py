"""Deployment recipes for HuggingFace models.

This module provides utilities for model quantization,
format conversion, and serving.

Examples:
    >>> from hf_gtc.deployment import ServerConfig, QuantizationType
    >>> config = ServerConfig(port=8080)
    >>> config.port
    8080
"""

from __future__ import annotations

from hf_gtc.deployment.gguf import (
    GGUFArchitecture,
    GGUFConfig,
    GGUFEndian,
    GGUFExportResult,
    GGUFMetadata,
    GGUFModelInfo,
    GGUFQuantType,
    create_gguf_config,
    create_gguf_export_result,
    create_gguf_metadata,
    estimate_gguf_size,
    format_gguf_export_result,
    format_gguf_model_info,
    get_gguf_architecture,
    get_gguf_config_dict,
    get_gguf_filename,
    get_gguf_metadata_dict,
    get_gguf_quant_type,
    get_recommended_gguf_quant,
    list_gguf_architectures,
    list_gguf_quant_types,
    validate_gguf_architecture,
    validate_gguf_config,
    validate_gguf_metadata,
    validate_gguf_quant_type,
)
from hf_gtc.deployment.merging import (
    DENSITY_VALUES,
    VALID_MERGE_METHODS,
    DensityType,
    MergeConfig,
    MergeMethod,
    MergeResult,
    ModelSlice,
    calculate_merged_parameter_count,
    create_merge_config,
    create_model_slice,
    estimate_merge_time,
    get_density_value,
    get_recommended_method,
    linear_interpolate,
    list_merge_methods,
    slerp,
    validate_merge_config,
    validate_model_slice,
)
from hf_gtc.deployment.optimization import (
    OptimizationResult,
    QuantizationConfig,
    QuantizationType,
    calculate_compression_ratio,
    estimate_model_size,
    get_model_loading_kwargs,
    get_optimization_result,
    get_quantization_config,
    list_quantization_types,
)
from hf_gtc.deployment.quantization import (
    AWQConfig,
    CalibrationConfig,
    CalibrationMethod,
    GPTQConfig,
    QuantGranularity,
    QuantMethod,
    QuantProfile,
    QuantResult,
    compute_compression_ratio,
    create_awq_config,
    create_calibration_config,
    create_gptq_config,
    create_quant_profile,
    create_quant_result,
    estimate_quantized_size,
    format_quant_result,
    get_awq_dict,
    get_calibration_method,
    get_gptq_dict,
    get_quant_granularity,
    get_quant_method,
    get_recommended_profile,
    list_calibration_methods,
    list_quant_granularities,
    list_quant_methods,
    validate_calibration_config,
    validate_calibration_method,
    validate_quant_granularity,
    validate_quant_method,
    validate_quant_profile,
)
from hf_gtc.deployment.safetensors import (
    VALID_DTYPES,
    VALID_TENSOR_FORMATS,
    DType,
    FileInfo,
    LoadConfig,
    SaveConfig,
    TensorFormat,
    TensorInfo,
    calculate_memory_savings,
    create_load_config,
    create_metadata,
    create_save_config,
    estimate_file_size,
    format_size,
    get_recommended_dtype,
    list_dtypes,
    list_tensor_formats,
    validate_load_config,
    validate_save_config,
    validate_tensor_name,
)
from hf_gtc.deployment.serving import (
    HealthStatus,
    InferenceBackend,
    InferenceRequest,
    InferenceResponse,
    ModelServer,
    ServerConfig,
    ServerStatus,
    compute_server_metrics,
    create_server,
    format_server_info,
    get_health_status,
    get_inference_backend,
    get_server_status,
    list_inference_backends,
    list_server_statuses,
    process_batch,
    process_request,
    start_server,
    stop_server,
    validate_inference_backend,
    validate_server_config,
    validate_server_status,
)

__all__: list[str] = [
    # Merging
    "DENSITY_VALUES",
    "VALID_DTYPES",
    "VALID_MERGE_METHODS",
    "VALID_TENSOR_FORMATS",
    # Quantization
    "AWQConfig",
    "CalibrationConfig",
    "CalibrationMethod",
    # SafeTensors
    "DType",
    "DensityType",
    "FileInfo",
    # GGUF
    "GGUFArchitecture",
    "GGUFConfig",
    "GGUFEndian",
    "GGUFExportResult",
    "GGUFMetadata",
    "GGUFModelInfo",
    "GGUFQuantType",
    "GPTQConfig",
    # Serving
    "HealthStatus",
    "InferenceBackend",
    "InferenceRequest",
    "InferenceResponse",
    "LoadConfig",
    "MergeConfig",
    "MergeMethod",
    "MergeResult",
    "ModelServer",
    "ModelSlice",
    "OptimizationResult",
    "QuantGranularity",
    "QuantMethod",
    "QuantProfile",
    "QuantResult",
    "QuantizationConfig",
    "QuantizationType",
    "SaveConfig",
    "ServerConfig",
    "ServerStatus",
    "TensorFormat",
    "TensorInfo",
    # Functions - Quantization
    "calculate_compression_ratio",
    # Functions - SafeTensors
    "calculate_memory_savings",
    # Functions - Merging
    "calculate_merged_parameter_count",
    "compute_compression_ratio",
    # Functions - Serving
    "compute_server_metrics",
    "create_awq_config",
    "create_calibration_config",
    # Functions - GGUF
    "create_gguf_config",
    "create_gguf_export_result",
    "create_gguf_metadata",
    "create_gptq_config",
    "create_load_config",
    "create_merge_config",
    "create_metadata",
    "create_model_slice",
    "create_quant_profile",
    "create_quant_result",
    "create_save_config",
    "create_server",
    "estimate_file_size",
    "estimate_gguf_size",
    "estimate_merge_time",
    "estimate_model_size",
    "estimate_quantized_size",
    "format_gguf_export_result",
    "format_gguf_model_info",
    "format_quant_result",
    "format_server_info",
    "format_size",
    "get_awq_dict",
    "get_calibration_method",
    "get_density_value",
    "get_gguf_architecture",
    "get_gguf_config_dict",
    "get_gguf_filename",
    "get_gguf_metadata_dict",
    "get_gguf_quant_type",
    "get_gptq_dict",
    "get_health_status",
    "get_inference_backend",
    "get_model_loading_kwargs",
    "get_optimization_result",
    "get_quant_granularity",
    "get_quant_method",
    "get_quantization_config",
    "get_recommended_dtype",
    "get_recommended_gguf_quant",
    "get_recommended_method",
    "get_recommended_profile",
    "get_server_status",
    "linear_interpolate",
    "list_calibration_methods",
    "list_dtypes",
    "list_gguf_architectures",
    "list_gguf_quant_types",
    "list_inference_backends",
    "list_merge_methods",
    "list_quant_granularities",
    "list_quant_methods",
    "list_quantization_types",
    "list_server_statuses",
    "list_tensor_formats",
    "process_batch",
    "process_request",
    "slerp",
    "start_server",
    "stop_server",
    "validate_calibration_config",
    "validate_calibration_method",
    "validate_gguf_architecture",
    "validate_gguf_config",
    "validate_gguf_metadata",
    "validate_gguf_quant_type",
    "validate_inference_backend",
    "validate_load_config",
    "validate_merge_config",
    "validate_model_slice",
    "validate_quant_granularity",
    "validate_quant_method",
    "validate_quant_profile",
    "validate_save_config",
    "validate_server_config",
    "validate_server_status",
    "validate_tensor_name",
]
