"""Evaluation recipes for HuggingFace models.

This module provides utilities for computing metrics,
running benchmarks, leaderboard integration, robustness testing,
model interpretability, bias detection, model editing, calibration,
uncertainty estimation, and evaluating model performance.

Examples:
    >>> from hf_gtc.evaluation import BenchmarkType, EvaluationMode
    >>> BenchmarkType.MMLU.value
    'mmlu'
    >>> EvaluationMode.ZERO_SHOT.value
    'zero_shot'
    >>> from hf_gtc.evaluation import BenchmarkConfig, LeaderboardConfig
    >>> config = BenchmarkConfig(benchmark_type=BenchmarkType.MMLU)
    >>> config.benchmark_type.value
    'mmlu'
    >>> lb_config = LeaderboardConfig(name="my-board")
    >>> lb_config.name
    'my-board'
    >>> from hf_gtc.evaluation import PerturbationType, RobustnessResult
    >>> PerturbationType.TYPO.value
    'typo'
    >>> from hf_gtc.evaluation import AttributionMethod, create_attribution_config
    >>> AttributionMethod.INTEGRATED_GRADIENTS.value
    'integrated_gradients'
    >>> from hf_gtc.evaluation import FairnessMetric, BiasDetectionConfig
    >>> FairnessMetric.DEMOGRAPHIC_PARITY.value
    'demographic_parity'
    >>> from hf_gtc.evaluation import ProfileMetric, ProfilingLevel
    >>> ProfileMetric.LATENCY.value
    'latency'
    >>> from hf_gtc.evaluation import EditingMethod, create_rome_config
    >>> EditingMethod.ROME.value
    'rome'
    >>> from hf_gtc.evaluation import CalibrationMethod, UncertaintyType
    >>> CalibrationMethod.TEMPERATURE.value
    'temperature'
    >>> UncertaintyType.ALEATORIC.value
    'aleatoric'
"""

from __future__ import annotations

from hf_gtc.evaluation.benchmarks import (
    VALID_BENCHMARK_TASKS,
    VALID_BENCHMARK_TYPES,
    VALID_EVALUATION_MODES,
    VALID_SCORING_METHODS,
    BenchmarkConfig,
    BenchmarkResult,
    BenchmarkRunner,
    BenchmarkStats,
    BenchmarkTask,
    BenchmarkType,
    EvaluationMode,
    HumanEvalConfig,
    LegacyBenchmarkConfig,
    LegacyBenchmarkResult,
    MMLUConfig,
    ScoringMethod,
    TimingResult,
    aggregate_benchmark_results,
    aggregate_results,
    calculate_benchmark_score,
    calculate_confidence_interval,
    compare_benchmark_results,
    compare_benchmarks,
    compute_percentile,
    compute_timing_stats,
    create_benchmark_config,
    create_benchmark_result,
    create_benchmark_runner,
    create_humaneval_config,
    create_mmlu_config,
    format_benchmark_result,
    format_benchmark_stats,
    get_benchmark_task,
    get_benchmark_type,
    get_evaluation_mode,
    get_recommended_benchmark_config,
    get_scoring_method,
    list_benchmark_tasks,
    list_benchmark_types,
    list_evaluation_modes,
    list_scoring_methods,
    run_benchmark,
    validate_benchmark_config,
    validate_benchmark_result,
    validate_benchmark_stats,
    validate_benchmark_task,
    validate_benchmark_type,
    validate_evaluation_mode,
    validate_humaneval_config,
    validate_legacy_benchmark_config,
    validate_mmlu_config,
    validate_scoring_method,
)
from hf_gtc.evaluation.bias import (
    BiasAuditResult,
    BiasDetectionConfig,
    BiasType,
    FairnessConstraint,
    FairnessMetric,
    MitigationStrategy,
    StereotypeConfig,
    calculate_demographic_parity,
    calculate_disparity_score,
    calculate_equalized_odds,
    create_bias_detection_config,
    create_fairness_constraint,
    create_stereotype_config,
    detect_stereotypes,
    format_bias_audit,
    get_bias_type,
    get_fairness_metric,
    get_mitigation_strategy,
    get_recommended_bias_config,
    list_bias_types,
    list_fairness_metrics,
    list_mitigation_strategies,
    validate_bias_detection_config,
    validate_bias_type,
    validate_fairness_constraint,
    validate_fairness_metric,
    validate_mitigation_strategy,
    validate_stereotype_config,
)
from hf_gtc.evaluation.calibration import (
    VALID_CALIBRATION_METHODS,
    VALID_CALIBRATION_METRICS,
    VALID_UNCERTAINTY_TYPES,
    CalibrationConfig,
    CalibrationMethod,
    CalibrationMetric,
    CalibrationStats,
    ReliabilityDiagram,
    TemperatureConfig,
    UncertaintyResult,
    UncertaintyType,
    calculate_brier_score,
    calculate_ece,
    compute_reliability_diagram,
    create_calibration_config,
    create_temperature_config,
    create_uncertainty_result,
    estimate_uncertainty,
    format_calibration_stats,
    get_calibration_method,
    get_calibration_metric,
    get_recommended_calibration_config,
    get_uncertainty_type,
    list_calibration_methods,
    list_calibration_metrics,
    list_uncertainty_types,
    optimize_temperature,
    validate_calibration_config,
    validate_calibration_method,
    validate_calibration_metric,
    validate_calibration_stats,
    validate_temperature_config,
    validate_uncertainty_result,
    validate_uncertainty_type,
)
from hf_gtc.evaluation.editing import (
    EditingConfig,
    EditingMethod,
    EditingStats,
    EditRequest,
    EditScope,
    LocalizationResult,
    LocalizationType,
    MEMITConfig,
    ROMEConfig,
    calculate_edit_success,
    create_edit_request,
    create_editing_config,
    create_memit_config,
    create_rome_config,
    format_editing_stats,
    get_edit_scope,
    get_editing_method,
    get_localization_type,
    get_recommended_editing_config,
    list_edit_scopes,
    list_editing_methods,
    list_localization_types,
    localize_knowledge,
    measure_generalization,
    measure_specificity,
    validate_edit_request,
    validate_edit_scope,
    validate_editing_config,
    validate_editing_method,
    validate_localization_type,
    validate_memit_config,
    validate_rome_config,
)
from hf_gtc.evaluation.interpretability import (
    AggregationMethod,
    AttentionConfig,
    AttributionConfig,
    AttributionMethod,
    InterpretabilityResult,
    VisualizationConfig,
    VisualizationType,
    aggregate_attention_weights,
    calculate_attribution_scores,
    create_attention_config,
    create_attribution_config,
    create_visualization_config,
    estimate_interpretation_time,
    format_interpretation_result,
    get_aggregation_method,
    get_attribution_method,
    get_recommended_interpretability_config,
    get_visualization_type,
    list_aggregation_methods,
    list_attribution_methods,
    list_visualization_types,
    validate_aggregation_method,
    validate_attention_config,
    validate_attribution_config,
    validate_attribution_method,
    validate_visualization_config,
    validate_visualization_type,
)
from hf_gtc.evaluation.leaderboards import (
    VALID_LEADERBOARD_TYPES,
    VALID_RANKING_METHODS,
    Leaderboard,
    LeaderboardCategory,
    LeaderboardConfig,
    LeaderboardEntry,
    LeaderboardStats,
    LeaderboardType,
    ModelScore,
    RankingMethod,
    SubmissionConfig,
    SubmissionResult,
    SubmissionStatus,
    add_entry,
    calculate_ranking,
    compare_entries,
    compare_models,
    compute_average_score,
    compute_elo_rating,
    compute_leaderboard_stats,
    create_leaderboard,
    create_leaderboard_config,
    create_leaderboard_entry,
    create_leaderboard_stats,
    create_submission,
    create_submission_config,
    filter_entries_by_size,
    find_entry_by_model,
    format_leaderboard,
    format_leaderboard_stats,
    format_submission,
    get_category,
    get_leaderboard_type,
    get_ranking_method,
    get_recommended_leaderboard_config,
    get_score_by_metric,
    get_top_entries,
    list_categories,
    list_leaderboard_types,
    list_ranking_methods,
    list_submission_statuses,
    parse_submission_result,
    validate_category,
    validate_leaderboard_config,
    validate_leaderboard_stats,
    validate_leaderboard_type,
    validate_ranking_method,
    validate_submission,
    validate_submission_config,
    validate_submission_status,
)
from hf_gtc.evaluation.metrics import (
    VALID_AGGREGATION_METHODS,
    VALID_METRIC_TYPES,
    VALID_ROUGE_VARIANTS,
    BERTScoreConfig,
    BLEUConfig,
    ClassificationMetrics,
    MetricConfig,
    MetricResult,
    MetricType,
    ROUGEConfig,
    RougeVariant,
    aggregate_metrics,
    calculate_bertscore,
    calculate_bleu,
    calculate_perplexity,
    calculate_rouge,
    compute_accuracy,
    compute_classification_metrics,
    compute_f1,
    compute_mean_loss,
    compute_perplexity,
    compute_precision,
    compute_recall,
    create_bertscore_config,
    create_bleu_config,
    create_compute_metrics_fn,
    create_metric_config,
    create_rouge_config,
    format_metric_stats,
    get_metric_type,
    get_recommended_metric_config,
    get_rouge_variant,
    list_metric_types,
    list_rouge_variants,
    validate_bertscore_config,
    validate_bleu_config,
    validate_metric_config,
    validate_metric_result,
    validate_metric_type,
    validate_rouge_config,
    validate_rouge_variant,
)
from hf_gtc.evaluation.metrics import (
    AggregationMethod as MetricAggregationMethod,
)
from hf_gtc.evaluation.metrics import (
    get_aggregation_method as get_metric_aggregation_method,
)
from hf_gtc.evaluation.metrics import (
    list_aggregation_methods as list_metric_aggregation_methods,
)
from hf_gtc.evaluation.metrics import (
    validate_aggregation_method as validate_metric_aggregation_method,
)
from hf_gtc.evaluation.profiling import (
    BottleneckType,
    LatencyBreakdown,
    MemoryBreakdown,
    ProfileMetric,
    ProfilingConfig,
    ProfilingLevel,
    ProfilingResult,
    calculate_flops,
    compare_profiles,
    create_latency_breakdown,
    create_memory_breakdown,
    create_profiling_config,
    create_profiling_result,
    estimate_memory_footprint,
    format_profiling_result,
    get_bottleneck_type,
    get_profile_metric,
    get_profiling_level,
    get_recommended_profiling_config,
    identify_bottlenecks,
    list_bottleneck_types,
    list_profile_metrics,
    list_profiling_levels,
    validate_bottleneck_type,
    validate_latency_breakdown,
    validate_memory_breakdown,
    validate_profile_metric,
    validate_profiling_config,
    validate_profiling_level,
    validate_profiling_result,
)
from hf_gtc.evaluation.robustness import (
    AdversarialConfig,
    AttackMethod,
    OODConfig,
    OODDetectionMethod,
    PerturbationConfig,
    PerturbationType,
    RobustnessResult,
    apply_perturbation,
    calculate_attack_success_rate,
    calculate_robustness_score,
    create_adversarial_config,
    create_ood_config,
    create_perturbation_config,
    detect_ood_samples,
    format_robustness_result,
    get_attack_method,
    get_ood_detection_method,
    get_perturbation_type,
    get_recommended_robustness_config,
    list_attack_methods,
    list_ood_detection_methods,
    list_perturbation_types,
    validate_adversarial_config,
    validate_ood_config,
    validate_perturbation_config,
)

__all__: list[str] = [
    "VALID_AGGREGATION_METHODS",
    "VALID_BENCHMARK_TASKS",
    "VALID_BENCHMARK_TYPES",
    "VALID_CALIBRATION_METHODS",
    "VALID_CALIBRATION_METRICS",
    "VALID_EVALUATION_MODES",
    "VALID_LEADERBOARD_TYPES",
    "VALID_METRIC_TYPES",
    "VALID_RANKING_METHODS",
    "VALID_ROUGE_VARIANTS",
    "VALID_SCORING_METHODS",
    "VALID_UNCERTAINTY_TYPES",
    "AdversarialConfig",
    "AggregationMethod",
    "AttackMethod",
    "AttentionConfig",
    "AttributionConfig",
    "AttributionMethod",
    "BERTScoreConfig",
    "BLEUConfig",
    "BenchmarkConfig",
    "BenchmarkResult",
    "BenchmarkRunner",
    "BenchmarkStats",
    "BenchmarkTask",
    "BenchmarkType",
    "BiasAuditResult",
    "BiasDetectionConfig",
    "BiasType",
    "BottleneckType",
    "CalibrationConfig",
    "CalibrationMethod",
    "CalibrationMetric",
    "CalibrationStats",
    "ClassificationMetrics",
    "EditRequest",
    "EditScope",
    "EditingConfig",
    "EditingMethod",
    "EditingStats",
    "EvaluationMode",
    "FairnessConstraint",
    "FairnessMetric",
    "HumanEvalConfig",
    "InterpretabilityResult",
    "LatencyBreakdown",
    "Leaderboard",
    "LeaderboardCategory",
    "LeaderboardConfig",
    "LeaderboardEntry",
    "LeaderboardStats",
    "LeaderboardType",
    "LegacyBenchmarkConfig",
    "LegacyBenchmarkResult",
    "LocalizationResult",
    "LocalizationType",
    "MEMITConfig",
    "MMLUConfig",
    "MemoryBreakdown",
    "MetricAggregationMethod",
    "MetricConfig",
    "MetricResult",
    "MetricType",
    "MitigationStrategy",
    "ModelScore",
    "OODConfig",
    "OODDetectionMethod",
    "PerturbationConfig",
    "PerturbationType",
    "ProfileMetric",
    "ProfilingConfig",
    "ProfilingLevel",
    "ProfilingResult",
    "ROMEConfig",
    "ROUGEConfig",
    "RankingMethod",
    "ReliabilityDiagram",
    "RobustnessResult",
    "RougeVariant",
    "ScoringMethod",
    "StereotypeConfig",
    "SubmissionConfig",
    "SubmissionResult",
    "SubmissionStatus",
    "TemperatureConfig",
    "TimingResult",
    "UncertaintyResult",
    "UncertaintyType",
    "VisualizationConfig",
    "VisualizationType",
    "add_entry",
    "aggregate_attention_weights",
    "aggregate_benchmark_results",
    "aggregate_metrics",
    "aggregate_results",
    "apply_perturbation",
    "calculate_attack_success_rate",
    "calculate_attribution_scores",
    "calculate_benchmark_score",
    "calculate_bertscore",
    "calculate_bleu",
    "calculate_brier_score",
    "calculate_confidence_interval",
    "calculate_demographic_parity",
    "calculate_disparity_score",
    "calculate_ece",
    "calculate_edit_success",
    "calculate_equalized_odds",
    "calculate_flops",
    "calculate_perplexity",
    "calculate_ranking",
    "calculate_robustness_score",
    "calculate_rouge",
    "compare_benchmark_results",
    "compare_benchmarks",
    "compare_entries",
    "compare_models",
    "compare_profiles",
    "compute_accuracy",
    "compute_average_score",
    "compute_classification_metrics",
    "compute_elo_rating",
    "compute_f1",
    "compute_leaderboard_stats",
    "compute_mean_loss",
    "compute_percentile",
    "compute_perplexity",
    "compute_precision",
    "compute_recall",
    "compute_reliability_diagram",
    "compute_timing_stats",
    "create_adversarial_config",
    "create_attention_config",
    "create_attribution_config",
    "create_benchmark_config",
    "create_benchmark_result",
    "create_benchmark_runner",
    "create_bertscore_config",
    "create_bias_detection_config",
    "create_bleu_config",
    "create_calibration_config",
    "create_compute_metrics_fn",
    "create_edit_request",
    "create_editing_config",
    "create_fairness_constraint",
    "create_humaneval_config",
    "create_latency_breakdown",
    "create_leaderboard",
    "create_leaderboard_config",
    "create_leaderboard_entry",
    "create_leaderboard_stats",
    "create_memit_config",
    "create_memory_breakdown",
    "create_metric_config",
    "create_mmlu_config",
    "create_ood_config",
    "create_perturbation_config",
    "create_profiling_config",
    "create_profiling_result",
    "create_rome_config",
    "create_rouge_config",
    "create_stereotype_config",
    "create_submission",
    "create_submission_config",
    "create_temperature_config",
    "create_uncertainty_result",
    "create_visualization_config",
    "detect_ood_samples",
    "detect_stereotypes",
    "estimate_interpretation_time",
    "estimate_memory_footprint",
    "estimate_uncertainty",
    "filter_entries_by_size",
    "find_entry_by_model",
    "format_benchmark_result",
    "format_benchmark_stats",
    "format_bias_audit",
    "format_calibration_stats",
    "format_editing_stats",
    "format_interpretation_result",
    "format_leaderboard",
    "format_leaderboard_stats",
    "format_metric_stats",
    "format_profiling_result",
    "format_robustness_result",
    "format_submission",
    "get_aggregation_method",
    "get_attack_method",
    "get_attribution_method",
    "get_benchmark_task",
    "get_benchmark_type",
    "get_bias_type",
    "get_bottleneck_type",
    "get_calibration_method",
    "get_calibration_metric",
    "get_category",
    "get_edit_scope",
    "get_editing_method",
    "get_evaluation_mode",
    "get_fairness_metric",
    "get_leaderboard_type",
    "get_localization_type",
    "get_metric_aggregation_method",
    "get_metric_type",
    "get_mitigation_strategy",
    "get_ood_detection_method",
    "get_perturbation_type",
    "get_profile_metric",
    "get_profiling_level",
    "get_ranking_method",
    "get_recommended_benchmark_config",
    "get_recommended_bias_config",
    "get_recommended_calibration_config",
    "get_recommended_editing_config",
    "get_recommended_interpretability_config",
    "get_recommended_leaderboard_config",
    "get_recommended_metric_config",
    "get_recommended_profiling_config",
    "get_recommended_robustness_config",
    "get_rouge_variant",
    "get_score_by_metric",
    "get_scoring_method",
    "get_top_entries",
    "get_uncertainty_type",
    "get_visualization_type",
    "identify_bottlenecks",
    "list_aggregation_methods",
    "list_attack_methods",
    "list_attribution_methods",
    "list_benchmark_tasks",
    "list_benchmark_types",
    "list_bias_types",
    "list_bottleneck_types",
    "list_calibration_methods",
    "list_calibration_metrics",
    "list_categories",
    "list_edit_scopes",
    "list_editing_methods",
    "list_evaluation_modes",
    "list_fairness_metrics",
    "list_leaderboard_types",
    "list_localization_types",
    "list_metric_aggregation_methods",
    "list_metric_types",
    "list_mitigation_strategies",
    "list_ood_detection_methods",
    "list_perturbation_types",
    "list_profile_metrics",
    "list_profiling_levels",
    "list_ranking_methods",
    "list_rouge_variants",
    "list_scoring_methods",
    "list_submission_statuses",
    "list_uncertainty_types",
    "list_visualization_types",
    "localize_knowledge",
    "measure_generalization",
    "measure_specificity",
    "optimize_temperature",
    "parse_submission_result",
    "run_benchmark",
    "validate_adversarial_config",
    "validate_aggregation_method",
    "validate_attention_config",
    "validate_attribution_config",
    "validate_attribution_method",
    "validate_benchmark_config",
    "validate_benchmark_result",
    "validate_benchmark_stats",
    "validate_benchmark_task",
    "validate_benchmark_type",
    "validate_bertscore_config",
    "validate_bias_detection_config",
    "validate_bias_type",
    "validate_bleu_config",
    "validate_bottleneck_type",
    "validate_calibration_config",
    "validate_calibration_method",
    "validate_calibration_metric",
    "validate_calibration_stats",
    "validate_category",
    "validate_edit_request",
    "validate_edit_scope",
    "validate_editing_config",
    "validate_editing_method",
    "validate_evaluation_mode",
    "validate_fairness_constraint",
    "validate_fairness_metric",
    "validate_humaneval_config",
    "validate_latency_breakdown",
    "validate_leaderboard_config",
    "validate_leaderboard_stats",
    "validate_leaderboard_type",
    "validate_legacy_benchmark_config",
    "validate_localization_type",
    "validate_memit_config",
    "validate_memory_breakdown",
    "validate_metric_aggregation_method",
    "validate_metric_config",
    "validate_metric_result",
    "validate_metric_type",
    "validate_mitigation_strategy",
    "validate_mmlu_config",
    "validate_ood_config",
    "validate_perturbation_config",
    "validate_profile_metric",
    "validate_profiling_config",
    "validate_profiling_level",
    "validate_profiling_result",
    "validate_ranking_method",
    "validate_rome_config",
    "validate_rouge_config",
    "validate_rouge_variant",
    "validate_scoring_method",
    "validate_stereotype_config",
    "validate_submission",
    "validate_submission_config",
    "validate_submission_status",
    "validate_temperature_config",
    "validate_uncertainty_result",
    "validate_uncertainty_type",
    "validate_visualization_config",
    "validate_visualization_type",
]
