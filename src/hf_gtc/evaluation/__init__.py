"""Evaluation recipes for HuggingFace models.

This module provides utilities for computing metrics,
running benchmarks, leaderboard integration, robustness testing,
model interpretability, bias detection, and evaluating model performance.

Examples:
    >>> from hf_gtc.evaluation import BenchmarkConfig, LeaderboardConfig
    >>> config = BenchmarkConfig(name="test")
    >>> config.name
    'test'
    >>> lb_config = LeaderboardConfig(name="my-board")
    >>> lb_config.name
    'my-board'
    >>> from hf_gtc.evaluation import PerturbationType, RobustnessResult
    >>> PerturbationType.TYPO.value
    'typo'
    >>> from hf_gtc.evaluation import AttributionMethod, create_attribution_config
    >>> AttributionMethod.INTEGRATED_GRADIENTS.value
    'integrated_gradients'
    >>> from hf_gtc.evaluation import FairnessMetric, BiasDetectionConfig
    >>> FairnessMetric.DEMOGRAPHIC_PARITY.value
    'demographic_parity'
"""

from __future__ import annotations

from hf_gtc.evaluation.benchmarks import (
    BenchmarkConfig,
    BenchmarkResult,
    BenchmarkRunner,
    BenchmarkTask,
    TimingResult,
    aggregate_benchmark_results,
    compare_benchmark_results,
    compute_percentile,
    compute_timing_stats,
    create_benchmark_runner,
    format_benchmark_result,
    get_benchmark_task,
    list_benchmark_tasks,
    run_benchmark,
    validate_benchmark_config,
    validate_benchmark_task,
)
from hf_gtc.evaluation.bias import (
    BiasAuditResult,
    BiasDetectionConfig,
    BiasType,
    FairnessConstraint,
    FairnessMetric,
    MitigationStrategy,
    StereotypeConfig,
    calculate_demographic_parity,
    calculate_disparity_score,
    calculate_equalized_odds,
    create_bias_detection_config,
    create_fairness_constraint,
    create_stereotype_config,
    detect_stereotypes,
    format_bias_audit,
    get_bias_type,
    get_fairness_metric,
    get_mitigation_strategy,
    get_recommended_bias_config,
    list_bias_types,
    list_fairness_metrics,
    list_mitigation_strategies,
    validate_bias_detection_config,
    validate_bias_type,
    validate_fairness_constraint,
    validate_fairness_metric,
    validate_mitigation_strategy,
    validate_stereotype_config,
)
from hf_gtc.evaluation.interpretability import (
    AggregationMethod,
    AttentionConfig,
    AttributionConfig,
    AttributionMethod,
    InterpretabilityResult,
    VisualizationConfig,
    VisualizationType,
    aggregate_attention_weights,
    calculate_attribution_scores,
    create_attention_config,
    create_attribution_config,
    create_visualization_config,
    estimate_interpretation_time,
    format_interpretation_result,
    get_aggregation_method,
    get_attribution_method,
    get_recommended_interpretability_config,
    get_visualization_type,
    list_aggregation_methods,
    list_attribution_methods,
    list_visualization_types,
    validate_aggregation_method,
    validate_attention_config,
    validate_attribution_config,
    validate_attribution_method,
    validate_visualization_config,
    validate_visualization_type,
)
from hf_gtc.evaluation.leaderboards import (
    Leaderboard,
    LeaderboardCategory,
    LeaderboardConfig,
    LeaderboardEntry,
    ModelScore,
    SubmissionResult,
    SubmissionStatus,
    add_entry,
    compare_entries,
    compute_average_score,
    compute_leaderboard_stats,
    create_leaderboard,
    create_submission,
    filter_entries_by_size,
    find_entry_by_model,
    format_leaderboard,
    get_category,
    get_score_by_metric,
    get_top_entries,
    list_categories,
    list_submission_statuses,
    parse_submission_result,
    validate_category,
    validate_leaderboard_config,
    validate_submission_status,
)
from hf_gtc.evaluation.metrics import (
    ClassificationMetrics,
    compute_accuracy,
    compute_classification_metrics,
    compute_f1,
    compute_mean_loss,
    compute_perplexity,
    compute_precision,
    compute_recall,
    create_compute_metrics_fn,
)
from hf_gtc.evaluation.robustness import (
    AdversarialConfig,
    AttackMethod,
    OODConfig,
    OODDetectionMethod,
    PerturbationConfig,
    PerturbationType,
    RobustnessResult,
    apply_perturbation,
    calculate_attack_success_rate,
    calculate_robustness_score,
    create_adversarial_config,
    create_ood_config,
    create_perturbation_config,
    detect_ood_samples,
    format_robustness_result,
    get_attack_method,
    get_ood_detection_method,
    get_perturbation_type,
    get_recommended_robustness_config,
    list_attack_methods,
    list_ood_detection_methods,
    list_perturbation_types,
    validate_adversarial_config,
    validate_ood_config,
    validate_perturbation_config,
)

__all__: list[str] = [
    "AdversarialConfig",
    "AggregationMethod",
    "AttackMethod",
    "AttentionConfig",
    "AttributionConfig",
    "AttributionMethod",
    "BenchmarkConfig",
    "BenchmarkResult",
    "BenchmarkRunner",
    "BenchmarkTask",
    "BiasAuditResult",
    "BiasDetectionConfig",
    "BiasType",
    "ClassificationMetrics",
    "FairnessConstraint",
    "FairnessMetric",
    "InterpretabilityResult",
    "Leaderboard",
    "LeaderboardCategory",
    "LeaderboardConfig",
    "LeaderboardEntry",
    "MitigationStrategy",
    "ModelScore",
    "OODConfig",
    "OODDetectionMethod",
    "PerturbationConfig",
    "PerturbationType",
    "RobustnessResult",
    "StereotypeConfig",
    "SubmissionResult",
    "SubmissionStatus",
    "TimingResult",
    "VisualizationConfig",
    "VisualizationType",
    "add_entry",
    "aggregate_attention_weights",
    "aggregate_benchmark_results",
    "apply_perturbation",
    "calculate_attack_success_rate",
    "calculate_attribution_scores",
    "calculate_demographic_parity",
    "calculate_disparity_score",
    "calculate_equalized_odds",
    "calculate_robustness_score",
    "compare_benchmark_results",
    "compare_entries",
    "compute_accuracy",
    "compute_average_score",
    "compute_classification_metrics",
    "compute_f1",
    "compute_leaderboard_stats",
    "compute_mean_loss",
    "compute_percentile",
    "compute_perplexity",
    "compute_precision",
    "compute_recall",
    "compute_timing_stats",
    "create_adversarial_config",
    "create_attention_config",
    "create_attribution_config",
    "create_benchmark_runner",
    "create_bias_detection_config",
    "create_compute_metrics_fn",
    "create_fairness_constraint",
    "create_leaderboard",
    "create_ood_config",
    "create_perturbation_config",
    "create_stereotype_config",
    "create_submission",
    "create_visualization_config",
    "detect_ood_samples",
    "detect_stereotypes",
    "estimate_interpretation_time",
    "filter_entries_by_size",
    "find_entry_by_model",
    "format_benchmark_result",
    "format_bias_audit",
    "format_interpretation_result",
    "format_leaderboard",
    "format_robustness_result",
    "get_aggregation_method",
    "get_attack_method",
    "get_attribution_method",
    "get_benchmark_task",
    "get_bias_type",
    "get_category",
    "get_fairness_metric",
    "get_mitigation_strategy",
    "get_ood_detection_method",
    "get_perturbation_type",
    "get_recommended_bias_config",
    "get_recommended_interpretability_config",
    "get_recommended_robustness_config",
    "get_score_by_metric",
    "get_top_entries",
    "get_visualization_type",
    "list_aggregation_methods",
    "list_attack_methods",
    "list_attribution_methods",
    "list_benchmark_tasks",
    "list_bias_types",
    "list_categories",
    "list_fairness_metrics",
    "list_mitigation_strategies",
    "list_ood_detection_methods",
    "list_perturbation_types",
    "list_submission_statuses",
    "list_visualization_types",
    "parse_submission_result",
    "run_benchmark",
    "validate_adversarial_config",
    "validate_aggregation_method",
    "validate_attention_config",
    "validate_attribution_config",
    "validate_attribution_method",
    "validate_benchmark_config",
    "validate_benchmark_task",
    "validate_bias_detection_config",
    "validate_bias_type",
    "validate_category",
    "validate_fairness_constraint",
    "validate_fairness_metric",
    "validate_leaderboard_config",
    "validate_mitigation_strategy",
    "validate_ood_config",
    "validate_perturbation_config",
    "validate_stereotype_config",
    "validate_submission_status",
    "validate_visualization_config",
    "validate_visualization_type",
]
